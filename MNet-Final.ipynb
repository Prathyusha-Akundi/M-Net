{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "from keras import objectives\n",
    "from keras.objectives import mae, categorical_crossentropy, mse\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
    "\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, UpSampling2D, ZeroPadding3D\n",
    "\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU, PReLU, LeakyReLU\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad, RMSprop,Adam\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adadelta, Adagrad, RMSprop,Adam\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from six.moves import range\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import h5py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = np.zeros(15)\n",
    "# for i in range(15):\n",
    "#     hist[i] = np.sum(train_labels==i)\n",
    "\n",
    "# ss = np.sum(hist).astype('float32')\n",
    "# class_weight = dict([(i, ss/hist[i]) for i in range(15)])\n",
    "# class_weight_list = [ss/hist[i] for i in range(15)]\n",
    "\n",
    "# print(class_weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_weight_list[0] = 50\n",
    "class_weight_list=[0.85, 7582.851282051282, 7375.864763752252, 2337.59072545231, 2493.8681658468026, 1092.9843336139459, 1132.4189163316103, 1058.3878317924248, 1054.8851809282232, 2350.0779656527307, 2263.0565428109853, 762.7289478586064, 777.3308411214954, 496.56358208955226, 500.8054792458511]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_to_categorical(y, nb_classes=14):\n",
    "    # Y = np.zeros([y.shape[0],y.shape[1],y.shape[2],y.max()+1],dtype='uint8')\n",
    "    # Y[np.nonzero(y)[0],np.nonzero(y)[1],np.nonzero(y)[2],y[np.nonzero(y)[0],np.nonzero(y)[1],np.nonzero(y)[2]]] = 1\n",
    "    # Y[np.where(y == 0)[0],np.where(y == 0)[1],np.where(y == 0)[2],y[np.where(y == 0)[0],np.where(y == 0)[1],np.where(y == 0)[2]]] = 1\n",
    "    for d in ['/device:GPU:0', '/device:GPU:1', '/device:GPU:2']:\n",
    "        with tf.device(d):\n",
    "            one_hot_y = tf.one_hot(indices = y, depth=nb_classes+1)\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "   \n",
    "    Y = sess.run(one_hot_y)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_cost(y_true, y_predicted):\n",
    "    class_weight_list_t = tf.convert_to_tensor(class_weight_list)\n",
    "    num_sum = 2.0 * ((K.sum(y_true * y_predicted * class_weight_list_t) + K.epsilon()))\n",
    "    den_sum =((K.sum(y_true) + K.sum(y_predicted)+ K.epsilon()))\n",
    "\n",
    "    return 1-(num_sum/den_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weighted_pixelwise_crossentropy(target, output):\n",
    "    #output = tf.clip_by_value(output, 10e-8, 1.-10e-8)\n",
    "    class_weight_list_t = tf.convert_to_tensor(class_weight_list)\n",
    "#     cw_loss =  -tf.reduce_sum(target * class_weight_list_t * tf.log(output))\n",
    "    dice_loss = dice_cost(target,output)\n",
    "    \n",
    "    #return (0.3*cw_loss + 0.7*dice_loss)\n",
    "    #return dice_loss\n",
    "    #init_l = tf.local_variables_initializer()\n",
    "    \n",
    "    \n",
    "    #fscore_loss = 1-fscore\n",
    "    #return (tf.nn.weighted_cross_entropy_with_logits(target,output,class_weight_list_t)) + dice_loss \n",
    "    return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1score_c(Y_pred, Y_true):\n",
    "    Y_pred = Y_pred.flatten()\n",
    "    Y_true = Y_true.flatten()\n",
    "    return f1_score(Y_pred,Y_true)\n",
    "\n",
    "def accuracy(Y_pre, Y_true):\n",
    "    accu = ((np.sum(Y_true[np.nonzero(Y_true)]==Y_pre[np.nonzero(Y_true)],dtype='float32'))/(np.count_nonzero(Y_true)))\n",
    "    zero_accu = ((np.sum(Y_true[np.where(Y_true == 0)]==Y_pre[np.where(Y_true == 0)],dtype='float32'))/(np.size(Y_true)-np.count_nonzero(Y_true)))\n",
    "    return [accu,zero_accu]\n",
    "\n",
    "\n",
    "smooth = 1.\n",
    "def dice_coef(y_true_f, y_pred_f):\n",
    "#     y_true_f = y_true.flatten()\n",
    "#     y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return ((2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth))*100\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 14\n",
    "img_w = 152\n",
    "img_h = 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((img_h, img_w, 51,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model building\n"
     ]
    }
   ],
   "source": [
    "print(\"Model building\")\n",
    "\n",
    "zp0 = ZeroPadding3D(padding=(3,3,0))(inputs)\n",
    "c3d = Conv3D(16,(7,7,51), activation='relu', kernel_initializer='glorot_uniform')(zp0)\n",
    "ac3d = Activation(\"relu\")(c3d)\n",
    "bnc3d = BatchNormalization(axis=-1)(ac3d)\n",
    "\n",
    "\n",
    "reshp = Reshape((img_w,img_h,16),input_shape=(img_w,img_h,1,16))(bnc3d)\n",
    "\n",
    "nd1 = MaxPooling2D((2,2), padding=\"valid\", data_format=\"channels_last\")(reshp)\n",
    "nd2 = MaxPooling2D((2,2), padding=\"valid\", data_format=\"channels_last\")(nd1)\n",
    "nd3 = MaxPooling2D((2,2), padding=\"valid\", data_format=\"channels_last\")(nd2)\n",
    "\n",
    "l11 = Conv2D(16,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(reshp)\n",
    "al11 = Activation(\"relu\")(l11)\n",
    "bnl11 = BatchNormalization()(al11)\n",
    "\n",
    "bnl11_reshp = concatenate([bnl11,reshp])\n",
    "l12 = Conv2D(32,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(bnl11_reshp)\n",
    "al12 = Activation(\"relu\")(l12)\n",
    "bnl12 = BatchNormalization()(al12)\n",
    "\n",
    "ld1 = Dropout(0.3)(bnl12)\n",
    "\n",
    "##################################\n",
    "\n",
    "lm1 = MaxPooling2D((2,2), padding=\"valid\", data_format=\"channels_last\")(ld1)\n",
    "\n",
    "##################################\n",
    "\n",
    "lm1_nd1 = concatenate([lm1,nd1])\n",
    "l21 = Conv2D(32,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(lm1_nd1)\n",
    "al21 = Activation(\"relu\")(l21)\n",
    "bnl21 = BatchNormalization()(al21)\n",
    "\n",
    "\n",
    "bnl21_lm1_nd1 = concatenate([bnl21,lm1,nd1])\n",
    "l22 = Conv2D(48,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(bnl21_lm1_nd1)\n",
    "al22 = Activation(\"relu\")(l22)\n",
    "bnl22 = BatchNormalization()(al22)\n",
    "\n",
    "ld2 = Dropout(0.3)(bnl22)\n",
    "\n",
    "##################################\n",
    "\n",
    "lm2 = MaxPooling2D((2,2))(ld2)\n",
    "\n",
    "##################################\n",
    "\n",
    "\n",
    "lm2_nd2 = concatenate([lm2,nd2])\n",
    "l31 = Conv2D(48,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(lm2_nd2)\n",
    "al31 = Activation(\"relu\")(l31)\n",
    "bnl31 = BatchNormalization()(al31)\n",
    "\n",
    "\n",
    "bnl31_lm2_nd2 = concatenate([bnl31,lm2,nd2])\n",
    "l32 = Conv2D(64,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(bnl31_lm2_nd2)\n",
    "al32 = Activation(\"relu\")(l32)\n",
    "bnl32 = BatchNormalization()(al32)\n",
    "\n",
    "ld3 = Dropout(0.3)(bnl32)\n",
    "\n",
    "\n",
    "##################################\n",
    "\n",
    "lm3 = MaxPooling2D((2,2), padding=\"valid\", data_format=\"channels_last\")(ld3)\n",
    "\n",
    "##################################\n",
    "\n",
    "lm3_nd3 = concatenate([lm3,nd3])\n",
    "l41 = Conv2D(64,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(lm3_nd3)\n",
    "al41 = Activation(\"relu\")(l41)\n",
    "bnl41 = BatchNormalization()(al41)\n",
    "\n",
    "\n",
    "bnl41_lm3_nd3 = concatenate([bnl41,lm3,nd3])\n",
    "n1 = Conv2D(128,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(bnl41_lm3_nd3)\n",
    "an1 = Activation(\"relu\")(n1)\n",
    "bnn1 = BatchNormalization()(an1)\n",
    "\n",
    "\n",
    "r41 = Conv2D(64,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(bnn1)\n",
    "ar41 = Activation(\"relu\")(r41)\n",
    "bnr41 = BatchNormalization()(ar41)\n",
    "\n",
    "rd3 = Dropout(0.3)(bnr41)\n",
    "\n",
    "######################################## DECODER ###########################################\n",
    "\n",
    "##################################\n",
    "ru3 = UpSampling2D((2,2), data_format=\"channels_last\")(rd3)\n",
    "#################################\n",
    "\n",
    "ru3_ld3_nd2 = concatenate([ru3,ld3,nd2])\n",
    "r32 = Conv2D(64,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(ru3_ld3_nd2)\n",
    "ar32 = Activation(\"relu\")(r32)\n",
    "bnr32 = BatchNormalization()(ar32)\n",
    "\n",
    "\n",
    "bnr32_ru3_ld3_nd2 = concatenate([bnr32,ru3,ld3,nd2])\n",
    "r31 = Conv2D(48,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(bnr32_ru3_ld3_nd2)\n",
    "ar31 = Activation(\"relu\")(r31)\n",
    "bnr31 = BatchNormalization()(ar31)\n",
    "\n",
    "rd2 = Dropout(0.3)(bnr31)\n",
    "\n",
    "##################################\n",
    "ru2 = UpSampling2D((2,2), data_format=\"channels_last\")(rd2)\n",
    "#################################\n",
    "\n",
    "ru2_ld2_nd1 = concatenate([ru2,ld2,nd1])\n",
    "r22 = Conv2D(48,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(ru2_ld2_nd1)\n",
    "ar22 = Activation(\"relu\")(r22)\n",
    "bnr22 = BatchNormalization()(ar22)\n",
    "\n",
    "\n",
    "bnr22_ru2_ld2_nd1 = concatenate([bnr22,ru2,ld2,nd1])\n",
    "r21 = Conv2D(32,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(bnr22_ru2_ld2_nd1)\n",
    "ar21 = Activation(\"relu\")(r21)\n",
    "bnr21 = BatchNormalization()(ar21)\n",
    "\n",
    "rd1 = Dropout(0.3)(bnr21)\n",
    "\n",
    "\n",
    "##################################\n",
    "ru1 = UpSampling2D((2,2), data_format=\"channels_last\")(rd1)\n",
    "#################################\n",
    "\n",
    "ru1_ld1_reshp = concatenate([ru1,ld1,reshp])\n",
    "r12 = Conv2D(32,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(ru1_ld1_reshp)\n",
    "ar12 = Activation(\"relu\")(r12)\n",
    "bnr12 = BatchNormalization()(ar12)\n",
    "\n",
    "\n",
    "bnr12_ru1_ld1_reshp = concatenate([bnr12,ru1,ld1,reshp])\n",
    "r11 = Conv2D(64,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(bnr12_ru1_ld1_reshp)\n",
    "ar11 = Activation(\"relu\")(r11)\n",
    "bnr11 = BatchNormalization()(ar11)\n",
    "\n",
    "rd01 = Dropout(0.3)(bnr11)\n",
    "\n",
    "\n",
    "################################\n",
    "ru2_ld2_nd1 = concatenate([ru2,ld2,nd1])\n",
    "ru2_2 = UpSampling2D((2,2), data_format=\"channels_last\")(ru2_ld2_nd1)\n",
    "\n",
    "ru3_ld3_nd2 = concatenate([ru3,ld3,nd2])\n",
    "ru3_2 = UpSampling2D((4,4), data_format=\"channels_last\")(ru3_ld3_nd2)\n",
    "\n",
    "#################################\n",
    "#'RD01','RU1','RU2_2','RU3_2','LD1','ReShp'\n",
    "lyrs = concatenate([rd01,ru1,ru2_2,ru3_2,ld1,reshp])\n",
    "r00 = Conv2D(200,(3,3), activation='relu', kernel_initializer='glorot_uniform', padding='same')(lyrs)\n",
    "ar00 = Activation(\"relu\")(r00)\n",
    "bnr00 = BatchNormalization()(ar00)\n",
    "\n",
    "rd02 = Dropout(0.3)(bnr00)\n",
    "\n",
    "\n",
    "lc = Conv2D(15,(1,1), activation='linear', kernel_initializer='glorot_uniform', padding='same')(rd02)\n",
    "output = Activation(\"softmax\")(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adm = Adam(lr=0.0001,beta_1=0.9,beta_2=0.999,epsilon=1e-08)\n",
    "#ada = Adadelta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiling\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 152, 152, 51, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding3d_1 (ZeroPadding3D (None, 158, 158, 51, 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 152, 152, 1,  40000       zero_padding3d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 152, 152, 1,  0           conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 152, 152, 1,  64          activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 152, 152, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 152, 152, 16) 2320        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 152, 152, 16) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 152, 152, 16) 64          activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 152, 152, 32) 0           batch_normalization_2[0][0]      \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 152, 152, 32) 9248        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 152, 152, 32) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 152, 152, 32) 128         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 152, 152, 32) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 76, 76, 32)   0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 76, 76, 16)   0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 76, 76, 48)   0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 76, 76, 32)   13856       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 76, 76, 32)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 76, 76, 32)   128         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 76, 76, 80)   0           batch_normalization_4[0][0]      \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 76, 76, 48)   34608       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 76, 76, 48)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 76, 76, 48)   192         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 76, 76, 48)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 38, 38, 48)   0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 16)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 38, 38, 64)   0           max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 38, 38, 48)   27696       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 38, 38, 48)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 38, 38, 48)   192         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 38, 38, 112)  0           batch_normalization_6[0][0]      \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 38, 38, 64)   64576       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 38, 38, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 38, 38, 64)   256         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 38, 38, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 19, 19, 64)   0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 19, 19, 16)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 19, 19, 80)   0           max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 19, 19, 64)   46144       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 19, 19, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 19, 19, 64)   256         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 19, 19, 144)  0           batch_normalization_8[0][0]      \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 19, 19, 128)  166016      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 19, 19, 128)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 19, 19, 128)  512         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 19, 19, 64)   73792       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 19, 19, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 19, 19, 64)   256         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 19, 19, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 38, 38, 64)   0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 38, 38, 144)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 38, 38, 64)   83008       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 38, 38, 64)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 38, 38, 64)   256         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 38, 38, 208)  0           batch_normalization_11[0][0]     \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 38, 38, 48)   89904       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 38, 38, 48)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 38, 38, 48)   192         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 38, 38, 48)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 76, 76, 48)   0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 76, 76, 112)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 76, 76, 48)   48432       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 76, 76, 48)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 76, 76, 48)   192         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 76, 76, 160)  0           batch_normalization_13[0][0]     \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 76, 76, 32)   46112       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 76, 76, 32)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 76, 76, 32)   128         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 76, 76, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 152, 152, 32) 0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 152, 152, 80) 0           up_sampling2d_3[0][0]            \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 152, 152, 32) 23072       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 152, 152, 32) 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 152, 152, 32) 128         activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 152, 152, 112 0           batch_normalization_15[0][0]     \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 152, 152, 64) 64576       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 152, 152, 64) 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 152, 152, 64) 256         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 76, 76, 112)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 38, 38, 144)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 152, 152, 64) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 152, 152, 112 0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 152, 152, 144 0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 152, 152, 400 0           dropout_7[0][0]                  \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 152, 152, 200 720200      concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 152, 152, 200 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 152, 152, 200 800         activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 152, 152, 200 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 152, 152, 15) 3015        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 152, 152, 15) 0           conv2d_17[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,560,575\n",
      "Trainable params: 1,558,575\n",
      "Non-trainable params: 2,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Model compiling\")\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "model.compile(optimizer=adm , loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11110\n",
      "11110\n"
     ]
    }
   ],
   "source": [
    "train_img, train_labels = preprocessing.get_data(partition=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.array(train_img)\n",
    "\n",
    "train_img = train_img.reshape(train_img.shape+(1,)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3703, 152, 152)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3703, 152, 152, 51, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proper Conversion to categorical\n"
     ]
    }
   ],
   "source": [
    "# print(\"Training data shape: {0}\".format(train_img.shape))\n",
    "# print(\"Training lables shape before : {0}\".format(train_labels.shape))\n",
    "training_gt_category = my_to_categorical(train_labels, nb_classes)\n",
    "\n",
    "\n",
    "#print(\"Training lables shape after : {0}\".format(training_gt_category.shape))\n",
    "\n",
    "if np.all(train_labels == np.argmax(training_gt_category,axis=-1)):\n",
    "    print('Proper Conversion to categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3703, 152, 152, 51, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3703, 152, 152, 15)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_gt_category.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = np.zeros((3353,15))\n",
    "for i in range(len(class_weight_list)):\n",
    "    class_weight[:,i] = class_weight_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2665 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2665/2665 [==============================] - 81s 30ms/step - loss: 1.7759 - acc: 0.7285 - val_loss: 0.2477 - val_acc: 0.9888\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24767, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2665/2665 [==============================] - 71s 27ms/step - loss: 0.7054 - acc: 0.9612 - val_loss: 0.6329 - val_acc: 0.9616\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.24767\n",
      "Epoch 3/30\n",
      "2665/2665 [==============================] - 71s 27ms/step - loss: 0.2574 - acc: 0.9832 - val_loss: 0.1633 - val_acc: 0.9910\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24767 to 0.16334, saving model to mnet_weights.h5\n",
      "Epoch 5/30\n",
      "2665/2665 [==============================] - 71s 27ms/step - loss: 0.1801 - acc: 0.9861 - val_loss: 0.0559 - val_acc: 0.9921\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16334 to 0.05590, saving model to mnet_weights.h5\n",
      "Epoch 6/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.1246 - acc: 0.9878 - val_loss: 0.0999 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.05590\n",
      "Epoch 7/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0943 - acc: 0.9886 - val_loss: 0.1181 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.05590\n",
      "Epoch 8/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0988 - acc: 0.9872 - val_loss: 0.0637 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.05590\n",
      "Epoch 9/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0741 - acc: 0.9884 - val_loss: 0.0730 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05590\n",
      "Epoch 10/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0523 - acc: 0.9899 - val_loss: 0.0481 - val_acc: 0.9921\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05590 to 0.04811, saving model to mnet_weights.h5\n",
      "Epoch 11/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0449 - acc: 0.9904 - val_loss: 0.0469 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04811 to 0.04690, saving model to mnet_weights.h5\n",
      "Epoch 12/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0392 - acc: 0.9909 - val_loss: 0.0255 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.04690 to 0.02548, saving model to mnet_weights.h5\n",
      "Epoch 13/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0348 - acc: 0.9914 - val_loss: 0.0226 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02548 to 0.02265, saving model to mnet_weights.h5\n",
      "Epoch 14/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0305 - acc: 0.9920 - val_loss: 0.1182 - val_acc: 0.9872\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02265\n",
      "Epoch 15/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0387 - acc: 0.9908 - val_loss: 0.0238 - val_acc: 0.9935\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02265\n",
      "Epoch 16/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0332 - acc: 0.9915 - val_loss: 0.0368 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02265\n",
      "Epoch 17/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0314 - acc: 0.9917 - val_loss: 0.0179 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02265 to 0.01794, saving model to mnet_weights.h5\n",
      "Epoch 18/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0258 - acc: 0.9926 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01794 to 0.01699, saving model to mnet_weights.h5\n",
      "Epoch 19/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0296 - acc: 0.9920 - val_loss: 0.0245 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01699\n",
      "Epoch 20/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0229 - acc: 0.9931 - val_loss: 0.0152 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01699 to 0.01522, saving model to mnet_weights.h5\n",
      "Epoch 21/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0209 - acc: 0.9936 - val_loss: 0.0164 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01522\n",
      "Epoch 22/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0243 - acc: 0.9930 - val_loss: 0.0155 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01522\n",
      "Epoch 23/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0196 - acc: 0.9939 - val_loss: 0.0148 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01522 to 0.01476, saving model to mnet_weights.h5\n",
      "Epoch 24/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0193 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01476\n",
      "Epoch 25/30\n",
      "2665/2665 [==============================] - 67s 25ms/step - loss: 0.0203 - acc: 0.9937 - val_loss: 0.0121 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01476 to 0.01208, saving model to mnet_weights.h5\n",
      "Epoch 26/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0170 - acc: 0.9945 - val_loss: 0.0125 - val_acc: 0.9959\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01208\n",
      "Epoch 27/30\n",
      "2665/2665 [==============================] - 67s 25ms/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0148 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01208\n",
      "Epoch 28/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0197 - acc: 0.9940 - val_loss: 0.0106 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01208 to 0.01062, saving model to mnet_weights.h5\n",
      "Epoch 29/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0168 - val_acc: 0.9949\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01062\n",
      "Epoch 30/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0163 - acc: 0.9947 - val_loss: 0.0100 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01062 to 0.01004, saving model to mnet_weights.h5\n",
      "741/741 [==============================] - 10s 14ms/step\n",
      "f1score.....\n",
      "0.7040630961891993\n",
      "acc_sc .....\n",
      "0.9950269461609489\n",
      "acc.....\n",
      "0.7050563536542976\n",
      "zacc.....\n",
      "0.9986283305709722\n",
      "Train on 2665 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0194 - acc: 0.9940 - val_loss: 0.0116 - val_acc: 0.9961\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01156, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2665/2665 [==============================] - 67s 25ms/step - loss: 0.0161 - acc: 0.9948 - val_loss: 0.0110 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01156 to 0.01096, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2665/2665 [==============================] - 67s 25ms/step - loss: 0.0157 - acc: 0.9950 - val_loss: 0.0105 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01096 to 0.01052, saving model to mnet_weights.h5\n",
      "Epoch 4/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0148 - acc: 0.9952 - val_loss: 0.0126 - val_acc: 0.9961\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01052\n",
      "Epoch 5/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0196 - acc: 0.9941 - val_loss: 0.0121 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01052\n",
      "Epoch 6/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0163 - acc: 0.9948 - val_loss: 0.0110 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01052\n",
      "Epoch 7/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0134 - acc: 0.9955 - val_loss: 0.0102 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01052 to 0.01024, saving model to mnet_weights.h5\n",
      "Epoch 8/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0137 - acc: 0.9954 - val_loss: 0.0103 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01024\n",
      "Epoch 9/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0128 - acc: 0.9957 - val_loss: 0.0094 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01024 to 0.00943, saving model to mnet_weights.h5\n",
      "Epoch 10/30\n",
      "2665/2665 [==============================] - 67s 25ms/step - loss: 0.0126 - acc: 0.9957 - val_loss: 0.0097 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00943\n",
      "Epoch 11/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.0139 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00943\n",
      "Epoch 12/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0160 - acc: 0.9948 - val_loss: 0.0098 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00943\n",
      "Epoch 13/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0125 - acc: 0.9957 - val_loss: 0.0100 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00943\n",
      "Epoch 14/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0138 - acc: 0.9954 - val_loss: 0.0089 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00943 to 0.00889, saving model to mnet_weights.h5\n",
      "Epoch 15/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0120 - acc: 0.9959 - val_loss: 0.0100 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00889\n",
      "Epoch 16/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0077 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00889 to 0.00771, saving model to mnet_weights.h5\n",
      "Epoch 17/30\n",
      "2665/2665 [==============================] - 67s 25ms/step - loss: 0.0123 - acc: 0.9958 - val_loss: 0.0523 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00771\n",
      "Epoch 18/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0116 - acc: 0.9960 - val_loss: 0.0110 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00771\n",
      "Epoch 19/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0120 - acc: 0.9960 - val_loss: 0.0093 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00771\n",
      "Epoch 20/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.0141 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00771\n",
      "Epoch 21/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0105 - acc: 0.9963 - val_loss: 0.1589 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00771\n",
      "Epoch 00021: early stopping\n",
      "741/741 [==============================] - 8s 10ms/step\n",
      "f1score.....\n",
      "0.7219133695523634\n",
      "acc_sc .....\n",
      "0.9950398549911963\n",
      "acc.....\n",
      "0.6939698404807382\n",
      "zacc.....\n",
      "0.9991273330077113\n",
      "Train on 2665 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2665/2665 [==============================] - 67s 25ms/step - loss: 0.0114 - acc: 0.9960 - val_loss: 0.2250 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22499, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0127 - acc: 0.9956 - val_loss: 0.0144 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.22499 to 0.01442, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0116 - acc: 0.9959 - val_loss: 0.0105 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01442 to 0.01053, saving model to mnet_weights.h5\n",
      "Epoch 4/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0108 - acc: 0.9961 - val_loss: 0.0091 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01053 to 0.00910, saving model to mnet_weights.h5\n",
      "Epoch 5/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0105 - acc: 0.9962 - val_loss: 0.0080 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00910 to 0.00798, saving model to mnet_weights.h5\n",
      "Epoch 6/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0109 - acc: 0.9962 - val_loss: 0.0524 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00798\n",
      "Epoch 7/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0126 - acc: 0.9957 - val_loss: 0.0117 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00798\n",
      "Epoch 8/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0130 - acc: 0.9955 - val_loss: 0.0081 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00798\n",
      "Epoch 9/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0109 - acc: 0.9961 - val_loss: 0.0078 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00798 to 0.00783, saving model to mnet_weights.h5\n",
      "Epoch 10/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0105 - acc: 0.9962 - val_loss: 0.0077 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00783 to 0.00774, saving model to mnet_weights.h5\n",
      "Epoch 11/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0102 - acc: 0.9963 - val_loss: 0.0073 - val_acc: 0.9974\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00774 to 0.00729, saving model to mnet_weights.h5\n",
      "Epoch 12/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0104 - acc: 0.9963 - val_loss: 0.0068 - val_acc: 0.9975\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00729 to 0.00681, saving model to mnet_weights.h5\n",
      "Epoch 13/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.0094 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00681\n",
      "Epoch 14/30\n",
      "2665/2665 [==============================] - 67s 25ms/step - loss: 0.0099 - acc: 0.9964 - val_loss: 0.0066 - val_acc: 0.9976\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00681 to 0.00659, saving model to mnet_weights.h5\n",
      "Epoch 15/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0082 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00659\n",
      "Epoch 16/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0086 - val_acc: 0.9970\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00659\n",
      "Epoch 17/30\n",
      "2665/2665 [==============================] - 68s 25ms/step - loss: 0.0094 - acc: 0.9966 - val_loss: 0.0083 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00659\n",
      "Epoch 18/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0102 - acc: 0.9963 - val_loss: 0.0086 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00659\n",
      "Epoch 19/30\n",
      "2665/2665 [==============================] - 68s 26ms/step - loss: 0.0099 - acc: 0.9964 - val_loss: 0.0075 - val_acc: 0.9974\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00659\n",
      "Epoch 00019: early stopping\n",
      "741/741 [==============================] - 8s 10ms/step\n",
      "f1score.....\n",
      "0.7989714639837726\n",
      "acc_sc .....\n",
      "0.9967623952807653\n",
      "acc.....\n",
      "0.7873983498865614\n",
      "zacc.....\n",
      "0.9990563052315506\n",
      "Train on 2666 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2666/2666 [==============================] - 70s 26ms/step - loss: 0.0098 - acc: 0.9964 - val_loss: 0.0075 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00751, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2666/2666 [==============================] - 68s 26ms/step - loss: 0.0095 - acc: 0.9965 - val_loss: 0.0071 - val_acc: 0.9975\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00751 to 0.00705, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2666/2666 [==============================] - 70s 26ms/step - loss: 0.0091 - acc: 0.9966 - val_loss: 0.0112 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00705\n",
      "Epoch 4/30\n",
      "2666/2666 [==============================] - 70s 26ms/step - loss: 0.0090 - acc: 0.9966 - val_loss: 0.0128 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00705\n",
      "Epoch 5/30\n",
      "2666/2666 [==============================] - 69s 26ms/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0084 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00705\n",
      "Epoch 6/30\n",
      "2666/2666 [==============================] - 71s 27ms/step - loss: 0.0091 - acc: 0.9966 - val_loss: 0.0078 - val_acc: 0.9974\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00705\n",
      "Epoch 7/30\n",
      "2666/2666 [==============================] - 69s 26ms/step - loss: 0.0089 - acc: 0.9967 - val_loss: 0.0102 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00705\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740/740 [==============================] - 7s 10ms/step\n",
      "f1score.....\n",
      "0.7917186727164787\n",
      "acc_sc .....\n",
      "0.9967800123530733\n",
      "acc.....\n",
      "0.7420693241684944\n",
      "zacc.....\n",
      "0.9994431459194097\n",
      "Train on 2666 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2666/2666 [==============================] - 67s 25ms/step - loss: 0.0087 - acc: 0.9968 - val_loss: 0.0079 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00792, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2666/2666 [==============================] - 70s 26ms/step - loss: 0.0083 - acc: 0.9968 - val_loss: 0.0069 - val_acc: 0.9974\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00792 to 0.00690, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2666/2666 [==============================] - 69s 26ms/step - loss: 0.0079 - acc: 0.9970 - val_loss: 0.0071 - val_acc: 0.9974\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00690\n",
      "Epoch 4/30\n",
      "2666/2666 [==============================] - 69s 26ms/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.0075 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00690\n",
      "Epoch 5/30\n",
      "2666/2666 [==============================] - 70s 26ms/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.0070 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00690\n",
      "Epoch 6/30\n",
      "2666/2666 [==============================] - 69s 26ms/step - loss: 0.0075 - acc: 0.9971 - val_loss: 0.0075 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00690\n",
      "Epoch 7/30\n",
      "2666/2666 [==============================] - 70s 26ms/step - loss: 0.0074 - acc: 0.9971 - val_loss: 0.0076 - val_acc: 0.9972\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00690\n",
      "Epoch 00007: early stopping\n",
      "740/740 [==============================] - 7s 9ms/step\n",
      "f1score.....\n",
      "0.8050467939552025\n",
      "acc_sc .....\n",
      "0.9969433162386764\n",
      "acc.....\n",
      "0.7423134581253625\n",
      "zacc.....\n",
      "0.9995628988651266\n"
     ]
    }
   ],
   "source": [
    "f1scores = []\n",
    "if(partition == 1):\n",
    "    for train, test in kf.split(train_img_n):\n",
    "        earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "        checkpointer = ModelCheckpoint('mnet_weights.h5', verbose=1, save_best_only=True)\n",
    "        results = model.fit(train_img_n[train], training_gt_category[train],epochs=30, validation_split=0.1, batch_size=8,callbacks=[earlystopper, checkpointer],class_weight = class_weight)\n",
    "        \n",
    "        pred = model.predict(train_img_n[test],verbose=1)\n",
    "\n",
    "        pred_t = np.argmax(pred,axis=-1).astype('uint8')\n",
    "\n",
    "        pred_t_flat = pred_t.flatten().astype('uint8')\n",
    "        test_gt_flat = np.argmax(training_gt_category[test],axis=-1).flatten().astype('uint8')\n",
    "\n",
    "        f1score = f1_score(test_gt_flat,pred_t_flat,average='macro')\n",
    "        print(\"f1score.....\")\n",
    "        print(f1score)\n",
    "        f1scores.append(f1score)\n",
    "        acc_sc = accuracy_score(test_gt_flat,pred_t_flat)\n",
    "        print(\"acc_sc .....\")\n",
    "        print(acc_sc)\n",
    "\n",
    "        acc,zacc = accuracy(pred_t_flat,test_gt_flat)\n",
    "        print(\"acc.....\")\n",
    "        print(acc)\n",
    "        print(\"zacc.....\")\n",
    "        print(zacc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7040630961891993,\n",
       " 0.7219133695523634,\n",
       " 0.7989714639837726,\n",
       " 0.7917186727164787,\n",
       " 0.8050467939552025]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7643426792794032"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042518930277585444"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2665 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2665/2665 [==============================] - 86s 32ms/step - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0062 - val_acc: 0.9975\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00624, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0065 - acc: 0.9976 - val_loss: 0.0057 - val_acc: 0.9978\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00624 to 0.00571, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0060 - acc: 0.9978 - val_loss: 0.0077 - val_acc: 0.9971\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00571\n",
      "Epoch 4/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 0.0059 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00571\n",
      "Epoch 5/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0054 - acc: 0.9980 - val_loss: 0.0057 - val_acc: 0.9978\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00571 to 0.00570, saving model to mnet_weights.h5\n",
      "Epoch 6/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0051 - acc: 0.9980 - val_loss: 0.0056 - val_acc: 0.9978\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00570 to 0.00557, saving model to mnet_weights.h5\n",
      "Epoch 7/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0050 - acc: 0.9981 - val_loss: 0.0067 - val_acc: 0.9974\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00557\n",
      "Epoch 8/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0050 - acc: 0.9981 - val_loss: 0.0058 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00557\n",
      "Epoch 9/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0047 - acc: 0.9982 - val_loss: 0.0051 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00557 to 0.00513, saving model to mnet_weights.h5\n",
      "Epoch 10/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0046 - acc: 0.9982 - val_loss: 0.0054 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00513\n",
      "Epoch 11/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0048 - acc: 0.9982 - val_loss: 0.0051 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00513\n",
      "Epoch 12/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0044 - acc: 0.9983 - val_loss: 0.0054 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00513\n",
      "Epoch 13/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0044 - acc: 0.9983 - val_loss: 0.0052 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00513\n",
      "Epoch 14/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0043 - acc: 0.9983 - val_loss: 0.0051 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00513 to 0.00510, saving model to mnet_weights.h5\n",
      "Epoch 15/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0042 - acc: 0.9984 - val_loss: 0.0053 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00510\n",
      "Epoch 16/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0041 - acc: 0.9984 - val_loss: 0.0053 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00510\n",
      "Epoch 17/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0042 - acc: 0.9984 - val_loss: 0.0050 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00510 to 0.00496, saving model to mnet_weights.h5\n",
      "Epoch 18/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0053 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00496\n",
      "Epoch 19/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0053 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00496\n",
      "Epoch 20/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0039 - acc: 0.9985 - val_loss: 0.0051 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00496\n",
      "Epoch 21/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0039 - acc: 0.9985 - val_loss: 0.0054 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00496\n",
      "Epoch 22/30\n",
      "2665/2665 [==============================] - 73s 28ms/step - loss: 0.0038 - acc: 0.9985 - val_loss: 0.0047 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00496 to 0.00468, saving model to mnet_weights.h5\n",
      "Epoch 23/30\n",
      "2665/2665 [==============================] - 74s 28ms/step - loss: 0.0038 - acc: 0.9985 - val_loss: 0.0050 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00468\n",
      "Epoch 24/30\n",
      "2665/2665 [==============================] - 74s 28ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.0048 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00468\n",
      "Epoch 25/30\n",
      "2665/2665 [==============================] - 74s 28ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.0048 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00468\n",
      "Epoch 26/30\n",
      "2665/2665 [==============================] - 74s 28ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.0049 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00468\n",
      "Epoch 27/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0036 - acc: 0.9986 - val_loss: 0.0050 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00468\n",
      "Epoch 00027: early stopping\n",
      "741/741 [==============================] - 11s 15ms/step\n",
      "f1score.....\n",
      "0.8424153571082866\n",
      "acc_sc .....\n",
      "0.9982750648595706\n",
      "acc.....\n",
      "0.8623478334894016\n",
      "zacc.....\n",
      "0.9994035019524004\n",
      "Train on 2665 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2665/2665 [==============================] - 69s 26ms/step - loss: 0.0038 - acc: 0.9985 - val_loss: 0.0050 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00505, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0036 - acc: 0.9986 - val_loss: 0.0050 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00505 to 0.00496, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0035 - acc: 0.9986 - val_loss: 0.0044 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00496 to 0.00443, saving model to mnet_weights.h5\n",
      "Epoch 4/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0035 - acc: 0.9986 - val_loss: 0.0046 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00443\n",
      "Epoch 5/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0051 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00443\n",
      "Epoch 6/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0056 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00443\n",
      "Epoch 7/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0044 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00443\n",
      "Epoch 8/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0046 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00443\n",
      "Epoch 00008: early stopping\n",
      "741/741 [==============================] - 7s 10ms/step\n",
      "f1score.....\n",
      "0.8973175474955976\n",
      "acc_sc .....\n",
      "0.9984983116885544\n",
      "acc.....\n",
      "0.9002059056200452\n",
      "zacc.....\n",
      "0.9995323408744523\n",
      "Train on 2665 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2665/2665 [==============================] - 70s 26ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.0050 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00499, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0036 - acc: 0.9986 - val_loss: 0.0048 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00499 to 0.00479, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0035 - acc: 0.9986 - val_loss: 0.0049 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00479\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0035 - acc: 0.9986 - val_loss: 0.0045 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00479 to 0.00447, saving model to mnet_weights.h5\n",
      "Epoch 5/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0051 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00447\n",
      "Epoch 6/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0034 - acc: 0.9987 - val_loss: 0.0047 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00447\n",
      "Epoch 7/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0050 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00447\n",
      "Epoch 8/30\n",
      "2665/2665 [==============================] - 72s 27ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0049 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00447\n",
      "Epoch 9/30\n",
      "2665/2665 [==============================] - 73s 27ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0048 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00447\n",
      "Epoch 00009: early stopping\n",
      "741/741 [==============================] - 8s 10ms/step\n",
      "f1score.....\n",
      "0.9056648509992172\n",
      "acc_sc .....\n",
      "0.9989555529698955\n",
      "acc.....\n",
      "0.9230269692107876\n",
      "zacc.....\n",
      "0.9995405309286961\n",
      "Train on 2666 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2666/2666 [==============================] - 73s 27ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0049 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00486, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2666/2666 [==============================] - 74s 28ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0047 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00486 to 0.00475, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2666/2666 [==============================] - 74s 28ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0046 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00475 to 0.00457, saving model to mnet_weights.h5\n",
      "Epoch 4/30\n",
      "2666/2666 [==============================] - 73s 28ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0051 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00457\n",
      "Epoch 5/30\n",
      "2666/2666 [==============================] - 73s 28ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0050 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00457\n",
      "Epoch 6/30\n",
      "2666/2666 [==============================] - 73s 27ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0045 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00457 to 0.00453, saving model to mnet_weights.h5\n",
      "Epoch 7/30\n",
      "2666/2666 [==============================] - 73s 28ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0048 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00453\n",
      "Epoch 8/30\n",
      "2666/2666 [==============================] - 73s 27ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0053 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00453\n",
      "Epoch 9/30\n",
      "2666/2666 [==============================] - 73s 28ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0048 - val_acc: 0.9981\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00453\n",
      "Epoch 10/30\n",
      "2666/2666 [==============================] - 73s 28ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0046 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00453\n",
      "Epoch 11/30\n",
      "2666/2666 [==============================] - 73s 27ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0053 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00453\n",
      "Epoch 00011: early stopping\n",
      "740/740 [==============================] - 7s 10ms/step\n",
      "f1score.....\n",
      "0.9031186763055731\n",
      "acc_sc .....\n",
      "0.9985596854645504\n",
      "acc.....\n",
      "0.914458228578494\n",
      "zacc.....\n",
      "0.9995255239706237\n",
      "Train on 2666 samples, validate on 297 samples\n",
      "Epoch 1/30\n",
      "2666/2666 [==============================] - 70s 26ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.0023 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00232, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2666/2666 [==============================] - 71s 27ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0024 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00232\n",
      "Epoch 3/30\n",
      "2666/2666 [==============================] - 72s 27ms/step - loss: 0.0031 - acc: 0.9988 - val_loss: 0.0024 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00232\n",
      "Epoch 4/30\n",
      "2666/2666 [==============================] - 71s 27ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0023 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00232\n",
      "Epoch 5/30\n",
      "2666/2666 [==============================] - 71s 27ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0024 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00232\n",
      "Epoch 6/30\n",
      "2666/2666 [==============================] - 72s 27ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0024 - val_acc: 0.9990\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00232\n",
      "Epoch 00006: early stopping\n",
      "740/740 [==============================] - 7s 9ms/step\n",
      "f1score.....\n",
      "0.901636130335163\n",
      "acc_sc .....\n",
      "0.9987624115632253\n",
      "acc.....\n",
      "0.9204788035150674\n",
      "zacc.....\n",
      "0.9994681380334067\n"
     ]
    }
   ],
   "source": [
    "f1scores = []\n",
    "if(partition != 1):\n",
    "    model2 = Model(inputs=[inputs], outputs=[output])\n",
    "    model2.load_weights('mnet_weights.h5')\n",
    "    model2.compile(optimizer=adm , loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    for train, test in kf.split(train_img_n):\n",
    "        earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "        checkpointer = ModelCheckpoint('mnet_weights.h5', verbose=1, save_best_only=True)\n",
    "        results = model2.fit(train_img_n[train], training_gt_category[train],epochs=30, validation_split=0.1, batch_size=8,callbacks=[earlystopper, checkpointer],class_weight = class_weight)\n",
    "        \n",
    "        pred = model2.predict(train_img_n[test],verbose=1)\n",
    "\n",
    "        pred_t = np.argmax(pred,axis=-1).astype('uint8')\n",
    "\n",
    "        pred_t_flat = pred_t.flatten().astype('uint8')\n",
    "        test_gt_flat = np.argmax(training_gt_category[test],axis=-1).flatten().astype('uint8')\n",
    "\n",
    "        f1score = f1_score(test_gt_flat,pred_t_flat,average='macro')\n",
    "        print(\"f1score.....\")\n",
    "        print(f1score)\n",
    "        f1scores.append(f1score)\n",
    "        acc_sc = accuracy_score(test_gt_flat,pred_t_flat)\n",
    "        print(\"acc_sc .....\")\n",
    "        print(acc_sc)\n",
    "\n",
    "        acc,zacc = accuracy(pred_t_flat,test_gt_flat)\n",
    "        print(\"acc.....\")\n",
    "        print(acc)\n",
    "        print(\"zacc.....\")\n",
    "        print(zacc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8900305124487675"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023961330678614435"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/30\n",
      "2160/2160 [==============================] - 64s 30ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.0040 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00404, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0037 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00404 to 0.00373, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.0040 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00373\n",
      "Epoch 4/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.0035 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00373 to 0.00352, saving model to mnet_weights.h5\n",
      "Epoch 5/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.0036 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00352\n",
      "Epoch 6/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.0034 - val_acc: 0.9986\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00352 to 0.00343, saving model to mnet_weights.h5\n",
      "Epoch 7/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0035 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00343\n",
      "Epoch 8/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0042 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00343\n",
      "Epoch 9/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0039 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00343\n",
      "Epoch 10/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0038 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00343\n",
      "Epoch 11/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00343\n",
      "Epoch 00011: early stopping\n",
      "600/600 [==============================] - 10s 16ms/step\n",
      "f1score.....\n",
      "0.8874774952011099\n",
      "acc_sc .....\n",
      "0.9986057969759926\n",
      "acc.....\n",
      "0.8949183913848224\n",
      "zacc.....\n",
      "0.9995025299158732\n",
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0040 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00396, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2160/2160 [==============================] - 58s 27ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0038 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00396 to 0.00379, saving model to mnet_weights.h5\n",
      "Epoch 3/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00379 to 0.00370, saving model to mnet_weights.h5\n",
      "Epoch 4/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0036 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00370 to 0.00358, saving model to mnet_weights.h5\n",
      "Epoch 5/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0038 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00358\n",
      "Epoch 6/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00358\n",
      "Epoch 7/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00358\n",
      "Epoch 8/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00358\n",
      "Epoch 9/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0038 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00358\n",
      "Epoch 00009: early stopping\n",
      "600/600 [==============================] - 8s 13ms/step\n",
      "f1score.....\n",
      "0.9083678705656822\n",
      "acc_sc .....\n",
      "0.998386354455217\n",
      "acc.....\n",
      "0.9167465950508344\n",
      "zacc.....\n",
      "0.99938074929846\n",
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/30\n",
      "2160/2160 [==============================] - 58s 27ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00372, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2160/2160 [==============================] - 58s 27ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0038 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00372\n",
      "Epoch 3/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0039 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00372\n",
      "Epoch 4/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00372 to 0.00371, saving model to mnet_weights.h5\n",
      "Epoch 5/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0039 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00371\n",
      "Epoch 6/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0051 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00371\n",
      "Epoch 7/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00371\n",
      "Epoch 8/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0042 - val_acc: 0.9983\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00371\n",
      "Epoch 9/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0038 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00371\n",
      "Epoch 00009: early stopping\n",
      "600/600 [==============================] - 6s 10ms/step\n",
      "f1score.....\n",
      "0.9207173162060531\n",
      "acc_sc .....\n",
      "0.9988390899122807\n",
      "acc.....\n",
      "0.9217354286091762\n",
      "zacc.....\n",
      "0.9996044049328592\n",
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/30\n",
      "2160/2160 [==============================] - 55s 25ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00365, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.0038 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00365\n",
      "Epoch 3/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0038 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00365\n",
      "Epoch 4/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0038 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00365\n",
      "Epoch 5/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0039 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00365\n",
      "Epoch 6/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0036 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00365 to 0.00361, saving model to mnet_weights.h5\n",
      "Epoch 7/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00361\n",
      "Epoch 8/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0039 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00361\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0043 - val_acc: 0.9984\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00361\n",
      "Epoch 10/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0037 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00361\n",
      "Epoch 11/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0038 - val_acc: 0.9985\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00361\n",
      "Epoch 00011: early stopping\n",
      "600/600 [==============================] - 5s 9ms/step\n",
      "f1score.....\n",
      "0.9288708594206911\n",
      "acc_sc .....\n",
      "0.99904706255771\n",
      "acc.....\n",
      "0.9483948551709714\n",
      "zacc.....\n",
      "0.9995410283906828\n",
      "Train on 2160 samples, validate on 240 samples\n",
      "Epoch 1/30\n",
      "2160/2160 [==============================] - 55s 25ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0028 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00277, saving model to mnet_weights.h5\n",
      "Epoch 2/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0028 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00277\n",
      "Epoch 3/30\n",
      "2160/2160 [==============================] - 56s 26ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0028 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00277\n",
      "Epoch 4/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0029 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00277\n",
      "Epoch 5/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.0028 - val_acc: 0.9988\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00277\n",
      "Epoch 6/30\n",
      "2160/2160 [==============================] - 57s 26ms/step - loss: 0.0028 - acc: 0.9989 - val_loss: 0.0030 - val_acc: 0.9987\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00277\n",
      "Epoch 00006: early stopping\n",
      "600/600 [==============================] - 5s 9ms/step\n",
      "f1score.....\n",
      "0.909237054478821\n",
      "acc_sc .....\n",
      "0.9989155557479225\n",
      "acc.....\n",
      "0.9167567111261186\n",
      "zacc.....\n",
      "0.9996236886947993\n"
     ]
    }
   ],
   "source": [
    "f1scores = []\n",
    "train_img_n1 = train_img_n[0:3000]\n",
    "training_gt_category1 = training_gt_category[0:3000]\n",
    "if(partition != 1):\n",
    "    model2 = Model(inputs=[inputs], outputs=[output])\n",
    "    model2.load_weights('mnet_weights.h5')\n",
    "    model2.compile(optimizer=adm , loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    for train, test in kf.split(train_img_n1):\n",
    "        earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "        checkpointer = ModelCheckpoint('mnet_weights.h5', verbose=1, save_best_only=True)\n",
    "        results = model2.fit(train_img_n1[train], training_gt_category1[train],epochs=30, validation_split=0.1, batch_size=8,callbacks=[earlystopper, checkpointer],class_weight = class_weight)\n",
    "        \n",
    "        pred = model2.predict(train_img_n1[test],verbose=1)\n",
    "\n",
    "        pred_t = np.argmax(pred,axis=-1).astype('uint8')\n",
    "\n",
    "        pred_t_flat = pred_t.flatten().astype('uint8')\n",
    "        test_gt_flat = np.argmax(training_gt_category1[test],axis=-1).flatten().astype('uint8')\n",
    "\n",
    "        f1score = f1_score(test_gt_flat,pred_t_flat,average='macro')\n",
    "        print(\"f1score.....\")\n",
    "        print(f1score)\n",
    "        f1scores.append(f1score)\n",
    "        acc_sc = accuracy_score(test_gt_flat,pred_t_flat)\n",
    "        print(\"acc_sc .....\")\n",
    "        print(acc_sc)\n",
    "\n",
    "        acc,zacc = accuracy(pred_t_flat,test_gt_flat)\n",
    "        print(\"acc.....\")\n",
    "        print(acc)\n",
    "        print(\"zacc.....\")\n",
    "        print(zacc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8874774952011099,\n",
       " 0.9083678705656822,\n",
       " 0.9207173162060531,\n",
       " 0.9288708594206911,\n",
       " 0.909237054478821]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9109341191744713"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013979388137898764"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(f1scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702/702 [==============================] - 8s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prathyuakundi/miniconda3/envs/tfenv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score.....\n",
      "0.8454961065113736\n",
      "acc_sc .....\n",
      "0.9989437701738602\n",
      "acc.....\n",
      "0.9219173007323815\n",
      "zacc.....\n",
      "0.9996161989779735\n"
     ]
    }
   ],
   "source": [
    "train_img_n2 = train_img_n[3000:-1]\n",
    "training_gt_category2 = training_gt_category[3000:-1]\n",
    "\n",
    "pred = model2.predict(train_img_n2,verbose=1)\n",
    "\n",
    "pred_t = np.argmax(pred,axis=-1).astype('uint8')\n",
    "\n",
    "pred_t_flat = pred_t.flatten().astype('uint8')\n",
    "test_gt_flat = np.argmax(training_gt_category2,axis=-1).flatten().astype('uint8')\n",
    "\n",
    "f1score = f1_score(test_gt_flat,pred_t_flat,average='macro')\n",
    "print(\"f1score.....\")\n",
    "print(f1score)\n",
    "f1scores.append(f1score)\n",
    "acc_sc = accuracy_score(test_gt_flat,pred_t_flat)\n",
    "print(\"acc_sc .....\")\n",
    "print(acc_sc)\n",
    "\n",
    "acc,zacc = accuracy(pred_t_flat,test_gt_flat)\n",
    "print(\"acc.....\")\n",
    "print(acc)\n",
    "print(\"zacc.....\")\n",
    "print(zacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(10,10))\n",
    "ax[0].imshow(np.argmax(training_gt_category2[700],axis=-1))\n",
    "ax[1].imshow(pred_t[700])\n",
    "ax[0].set_title('Ground truth')\n",
    "ax[1].set_title('Predicted')\n",
    "ax[0].set_xticks([])\n",
    "ax[1].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(10,10))\n",
    "ax[0].imshow(np.argmax(training_gt_category2[250],axis=-1))\n",
    "ax[1].imshow(pred_t[250])\n",
    "ax[0].set_title('Ground truth')\n",
    "ax[1].set_title('Predicted')\n",
    "ax[0].set_xticks([])\n",
    "ax[1].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(10,10))\n",
    "ax[0].imshow(np.argmax(training_gt_category2[520],axis=-1))\n",
    "ax[1].imshow(pred_t[520])\n",
    "ax[0].set_title('Ground truth')\n",
    "ax[1].set_title('Predicted')\n",
    "ax[0].set_xticks([])\n",
    "ax[1].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(10,10))\n",
    "ax[0].imshow(np.argmax(training_gt_category2[540],axis=-1))\n",
    "ax[1].imshow(pred_t[540])\n",
    "ax[0].set_title('Ground truth')\n",
    "ax[1].set_title('Predicted')\n",
    "ax[0].set_xticks([])\n",
    "ax[1].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['acc'])\n",
    "plt.plot(results.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
